{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.utils as utils\n",
    "\n",
    "# Initialize environment.\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "utils.init_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local libraries\n",
    "import data.query as dq\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier #, VotingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "from skmultiflow.trees import RegressionHoeffdingTree, RegressionHAT, HoeffdingTree, HAT\n",
    "\n",
    "from lib.bagging import BaggingRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "random_seed=1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "DATE_FROM = '2010-01-01'\n",
    "DATE_TO = '2017-12-31'\n",
    "TEST_STATIONS = [\n",
    "    5078,  # Ljubljanica (Moste)\n",
    "#     5040,  # Ljubljanica (Kamin)\n",
    "#     8080,  # Soča (Kobarid) !\n",
    "#     8180,  # Soča (Solkan) !\n",
    "#     8454,  # Cerknica\n",
    "]\n",
    "DROP_FEATURES = {\n",
    "#     'day_time',\n",
    "#     'precipitation',\n",
    "#     'snow_accumulation',\n",
    "#     'temperature_avg',\n",
    "#     'temperature_min',\n",
    "#     'temperature_max',\n",
    "#     'cloud_cover_avg',\n",
    "#     'cloud_cover_min',\n",
    "#     'cloud_cover_max',\n",
    "#     'dew_point_avg', \n",
    "#     'dew_point_min', \n",
    "#     'dew_point_max',\n",
    "#     'humidity_avg',\n",
    "#     'humidity_min',\n",
    "#     'humidity_max',\n",
    "#     'pressure_avg', \n",
    "#     'pressure_min', \n",
    "#     'pressure_max',\n",
    "    'uv_index_avg', \n",
    "    'uv_index_min', \n",
    "    'uv_index_max',\n",
    "#     'precipitation_probability_avg',\n",
    "#     'precipitation_probability_min',\n",
    "#     'precipitation_probability_max',\n",
    "#     'precipitation_intensity_avg',\n",
    "#     'precipitation_intensity_min',\n",
    "#     'precipitation_intensity_max'\n",
    "}\n",
    "IMPUTATE = True\n",
    "\n",
    "# Modelling types\n",
    "MODEL_REG = 'regression'\n",
    "MODEL_CLS = 'classification'\n",
    "\n",
    "# Processing types\n",
    "PROCESS_BAT = 'batch'\n",
    "PROCESS_STR = 'stream'\n",
    "\n",
    "# Prediction value type\n",
    "PREDICT_ABS = 'absolute'\n",
    "PREDICT_REL = 'relative'\n",
    "\n",
    "# Methods\n",
    "METHODS = {\n",
    "    MODEL_REG: {\n",
    "        PROCESS_BAT: {\n",
    "            'LINR': {\n",
    "                'name': 'Linear Regression',\n",
    "                'class': LinearRegression,\n",
    "                'params': {},\n",
    "            },\n",
    "            'DTR': {\n",
    "                'name': 'Decision Tree Regressor',\n",
    "                'class': DecisionTreeRegressor,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'splitter': [\"random\", \"best\"],\n",
    "                         'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'RFR': {\n",
    "                'name': 'Random Forest Regressor',\n",
    "                'class': RandomForestRegressor,\n",
    "                'params': {'n_estimators':10, 'random_state':random_seed},\n",
    "                'grid': {'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None],\n",
    "                         'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'GBR': {\n",
    "                'name': 'Gradient Boosting Regressor',\n",
    "                'class': GradientBoostingRegressor,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None],\n",
    "                         'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'PLSR': {\n",
    "                'name': 'PLS Regression',\n",
    "                'class': PLSRegression,\n",
    "                'params': {},\n",
    "            },\n",
    "            'ETR': {\n",
    "                'name': 'Extra Tree Regressor',\n",
    "                'class': ExtraTreeRegressor,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'splitter': [\"random\", \"best\"],\n",
    "                         'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None],\n",
    "                         'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'SVR': {\n",
    "                'name': 'SVR',\n",
    "                'class': SVR,\n",
    "                'params': {\n",
    "                    'gamma': 'auto',\n",
    "                },\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "            'MLPR': {\n",
    "                'name': 'Multi-Layer Perceptron Regressor',\n",
    "                'class': MLPRegressor,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'hidden_layer_sizes': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]},\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "            'KNR': {\n",
    "                'name': 'k Nearest Neighbors Regressor',\n",
    "                'class': KNeighborsRegressor,\n",
    "                'params': {},\n",
    "                'grid': {'n_neighbors': [3, 5, 7, 13]},\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        PROCESS_STR: {\n",
    "            'RHT': {\n",
    "                'name': 'Hoeffding Tree Regressor',\n",
    "                'class': RegressionHoeffdingTree,\n",
    "                'params': {\n",
    "                    'leaf_prediction': 'perceptron',\n",
    "                     'random_state': 0\n",
    "                }\n",
    "            },\n",
    "            'RHAT': {\n",
    "                'name': 'Hoeffding Adaptive Tree Regressor',\n",
    "                'class': RegressionHAT,\n",
    "                'params': {\n",
    "                    'leaf_prediction': 'perceptron',\n",
    "                     'random_state': 0\n",
    "                },\n",
    "            },\n",
    "            'BAGRHT': {\n",
    "                'name': 'Bagging (Hoeffding Tree) Regressor',\n",
    "                'class': BaggingRegression,\n",
    "                'params': {\n",
    "                    'base_estimator': {\n",
    "                        'name': 'Hoeffding Tree',\n",
    "                        'class': RegressionHoeffdingTree,\n",
    "                        'params': {\n",
    "                            'leaf_prediction': 'perceptron',\n",
    "                            'random_state': 0\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            'BAGRHAT': {\n",
    "                'name': 'Bagging (Hoeffding Adaptive Tree) Regressor',\n",
    "                'class': BaggingRegression,\n",
    "                'params': {\n",
    "                    'base_estimator': {\n",
    "                        'class': RegressionHAT,\n",
    "                        'params': {\n",
    "                            'leaf_prediction': 'perceptron',\n",
    "                            'random_state': 0\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    MODEL_CLS: {\n",
    "        PROCESS_BAT: {\n",
    "            'LOGR': {\n",
    "                'name': 'Logistic Regression',\n",
    "                'class': LogisticRegression,\n",
    "                'params': {'solver':'liblinear', 'multi_class':'auto', 'random_state':random_seed},\n",
    "                'scaler': None,\n",
    "            },\n",
    "            'DTC': {\n",
    "                'name': 'Decision Tree Classifier',\n",
    "                'class': DecisionTreeClassifier,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'splitter': [\"random\", \"best\"],\n",
    "                         'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'ETC': {\n",
    "                'name': 'Extra Tree Classifier',\n",
    "                'class': ExtraTreeClassifier,\n",
    "                'params': {'random_state':random_seed},\n",
    "                'grid': {'splitter': [\"random\", \"best\"],\n",
    "                         'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None],\n",
    "                         'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'RFC': {\n",
    "                'name': 'Random Forest Classifier',\n",
    "                'class': RandomForestClassifier,\n",
    "                'params': {'random_state':random_seed, 'n_estimators': 10},\n",
    "                'grid': {'max_depth': [2, 3, 5, 7, None],\n",
    "                         'max_features': [\"sqrt\", \"auto\", \"auto\", None],\n",
    "                         'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "                        },\n",
    "                'scaler': None\n",
    "            },\n",
    "            'SVC': {\n",
    "                'name': 'SVC',\n",
    "                'class': SVC,\n",
    "                'params': {'gamma':'auto', 'random_state': random_seed},\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "            'KNC': {\n",
    "                'name': 'k Nearest Neighbors Classifier',\n",
    "                'class': KNeighborsClassifier,\n",
    "                'params': {},\n",
    "                'grid': {'n_neighbors': [3, 5, 7, 13]},\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "            'PERC': {\n",
    "                'name': 'Perceptron Classifier',\n",
    "                'class': Perceptron,\n",
    "                'params': {'random_state': random_seed},\n",
    "                'scaler': {\n",
    "                    'class': MinMaxScaler,\n",
    "                },\n",
    "            },\n",
    "            'MNB': {\n",
    "                'name': 'Multinominal Naïve Bayes Classifier',\n",
    "                'class': MultinomialNB,\n",
    "                'params': {},\n",
    "            },\n",
    "            'GNB': {\n",
    "                'name': 'Gaussian Naïve Bayes Classifier',\n",
    "                'class': GaussianNB,\n",
    "                'params': {},\n",
    "            },\n",
    "        },\n",
    "        PROCESS_STR: {\n",
    "            'HT': {\n",
    "                'name': 'Hoeffding Tree Classifier',\n",
    "                'class': HoeffdingTree,\n",
    "                'params': {},\n",
    "            },\n",
    "            'HAT': {\n",
    "                'name': 'Hoeffding Adaptive Tree Classifier',\n",
    "                'class': HAT,\n",
    "                'params': {},\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "MAX_FEATURES = 120\n",
    "HORIZON = 3\n",
    "MAX_SHIFT = 20\n",
    "MAX_AVERAGE = 20\n",
    "FEATURE_SELECTION_ALGORITHM = \"GENETIC\"\n",
    "\n",
    "# Testing\n",
    "TEST_SIZE = 0.25\n",
    "TEST_TRIALS = 5\n",
    "\n",
    "# Evaluation\n",
    "EVAL_SCORE = 'score'\n",
    "EVAL_ERROR = 'error'\n",
    "EVAL = {\n",
    "    MODEL_REG: {\n",
    "        EVAL_SCORE: {\n",
    "            'class': r2_score,\n",
    "            'params': {}\n",
    "        }\n",
    "    },\n",
    "    MODEL_CLS: {\n",
    "        EVAL_SCORE: {\n",
    "            'class': f1_score,\n",
    "            'params': {\n",
    "                'average': 'macro'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather(dataset):\n",
    "    if 'day_time' in dataset:\n",
    "        dataset['day_time'] /= 3600\n",
    "    \n",
    "\n",
    "def imputate(df):\n",
    "    \"\"\"Interpolate given data frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Data frame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Data frame with interpolated missing values.\n",
    "    \n",
    "    \"\"\"\n",
    "    df0 = df.interpolate(method='spline', order=2)\n",
    "    df0[df0.isna()] = 0.0\n",
    "    return df0\n",
    "\n",
    "\n",
    "def create_results_df(datasets):\n",
    "    \"\"\"Generate empty dataframe for evaluation results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list[pandas.DataFrame]\n",
    "        List of datasets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Dataframe\n",
    "        Empty data frame for evaluation results.\n",
    "\n",
    "    \"\"\"\n",
    "    index = list(itertools.product(\n",
    "                datasets.keys(),\n",
    "                [PREDICT_ABS, PREDICT_REL],\n",
    "                [MODEL_REG, MODEL_CLS],\n",
    "                [PROCESS_BAT, PROCESS_STR]))\n",
    "    extended_index = []\n",
    "    columns=['Score', 'Score Min', 'Score Max']\n",
    "    if HORIZON > 0:\n",
    "        columns = ['Score', 'Score Min', 'Score Max', f'Score (h={HORIZON})', f'Score Min (h={HORIZON})', f'Score Max (h={HORIZON})', 'Learning Time', 'Predicting Time']\n",
    "    else:\n",
    "        columns = ['Score', 'Score Min', 'Score Max', 'Learning Time', 'Predicting Time']\n",
    "    for t in index:\n",
    "        for key, method in METHODS[t[2]][t[3]].items():\n",
    "            print(\"METHODS[{}][{}] [{}][{}]\".format(t[2], t[3], key, method)) # TODO delete\n",
    "            m = method['class']()\n",
    "            extended_index.append(t + (f'{type(m).__name__}[{key}]',))\n",
    "    return pd.DataFrame(\n",
    "        data=[],\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            extended_index,\n",
    "            names=['Station', 'Value', 'Modelling', 'Processing', 'Method']),\n",
    "        columns=columns\n",
    "    ).sort_index()\n",
    "\n",
    "\n",
    "def get_range(min, max):\n",
    "    \"\"\"Get a range between `min` and `max` with predetermined step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min : int\n",
    "        Lower bound of a range.\n",
    "        \n",
    "    max : int\n",
    "        Upper bound of a range.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Rane of integers between given bounds.\n",
    "    \n",
    "    \"\"\"\n",
    "    r = range(min, max)\n",
    "    r = []\n",
    "    i = min\n",
    "    while i <= max:\n",
    "        r.append(i)\n",
    "        e = math.floor(math.log10(i))\n",
    "        d = 10 ** e\n",
    "        if i < 10 ** (e + 1) / 2:\n",
    "            d = math.ceil(d / 2)\n",
    "        i += d\n",
    "    return r\n",
    "\n",
    "\n",
    "def shift_features(dataset, values, max_shift=MAX_SHIFT, horizon=HORIZON):\n",
    "    days = get_range(1, max_shift)\n",
    "    for feature_name in list(dataset.columns):\n",
    "#         if feature_name[-4:] in ['_min', '_max']:\n",
    "#             continue\n",
    "        for i in days:\n",
    "            if feature_name in values and i < horizon:\n",
    "                continue\n",
    "            dataset[f'{feature_name}_shift_{str(i)}d'] = dataset[feature_name].shift(i)\n",
    "\n",
    "\n",
    "def average_features(dataset, values, max_average=MAX_AVERAGE):\n",
    "    days = get_range(2, max_average)\n",
    "    for feature_name in list(dataset.columns):\n",
    "#         if feature_name[-4:] in ['_min', '_max']:\n",
    "#             continue\n",
    "        if feature_name in values:\n",
    "            continue\n",
    "        for i in days:\n",
    "            dataset[f'{feature_name}_average_{str(i)}d'] = dataset[feature_name].rolling(i).sum() / i\n",
    "\n",
    "\n",
    "def select_features(features, max_variations=3, max_features=100):\n",
    "    feature_map = {}\n",
    "    features.sort(key=lambda f: f[1], reverse=True)\n",
    "    for feature_name, feature_score in features:\n",
    "        feature_match = re.match(r'(.*)_\\d+d', feature_name)\n",
    "        if feature_match:\n",
    "            feature_base = feature_match.group(1)\n",
    "            if feature_base not in feature_map:\n",
    "                feature_map[feature_base] = {\n",
    "                    feature_name: feature_score\n",
    "                }\n",
    "            elif len(feature_map[feature_base].items()) < max_variations:\n",
    "                feature_map[feature_base][feature_name] = feature_score\n",
    "        else:\n",
    "            feature_map[feature_name] = feature_score\n",
    "\n",
    "    selected_features = []\n",
    "    for key, value in feature_map.items():\n",
    "        if isinstance(value, float):\n",
    "            selected_features.append((key, value))\n",
    "        else:\n",
    "            for sub_key, sub_value in value.items():\n",
    "                selected_features.append((sub_key, sub_value))\n",
    "                \n",
    "#     counters = {\n",
    "#         'variation': {},\n",
    "#         'base': {}\n",
    "#     }\n",
    "    \n",
    "#     for feature_name, feature_score in features:\n",
    "#         feature_match = re.match(r'(.*)_\\d+d', feature_name)\n",
    "#         if feature_match:\n",
    "\n",
    "    selected_features.sort(key=lambda f: f[1], reverse=True)\n",
    "    \n",
    "    if len(selected_features) > max_features:\n",
    "        selected_features = selected_features[:max_features]\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def filter_columns(dataset, columns):\n",
    "    ds = pd.DataFrame()\n",
    "    for col in columns:\n",
    "        if col in dataset.columns:\n",
    "            ds[col] = dataset[col]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def discretize_abs(df):\n",
    "    # TODO\n",
    "    df = df / 10\n",
    "    df = df.round(decimals=0)\n",
    "    df = df * 10\n",
    "    return df.astype(np.int)\n",
    "\n",
    "\n",
    "def discretize_rel(df):\n",
    "    # TODO\n",
    "    df = df / 5\n",
    "    df = df.round(decimals=0)\n",
    "    df = df * 5\n",
    "    return df.astype(np.int)\n",
    "\n",
    "\n",
    "def save_figure(plt, file_name):\n",
    "    plt.savefig(f'resources/{file_name}', dpi = 300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def plot_corr(corr, corr_labels, size=6, file_name=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    im = plt.imshow(corr)\n",
    "\n",
    "    # Colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "    corr_ticks = range(len(corr_labels))\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(len(corr_labels)))\n",
    "    ax.set_yticks(np.arange(len(corr_labels)))\n",
    "    ax.set_xticklabels(corr_labels)\n",
    "    ax.set_yticklabels(corr_labels)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha='left', va='center', rotation_mode='anchor', \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top')\n",
    "    \n",
    "    if file_name:\n",
    "        save_figure(plt, file_name)\n",
    "    \n",
    "\n",
    "def cluster_df(df, k=0.5):\n",
    "    X = df.corr().values\n",
    "    d = sch.distance.pdist(X)\n",
    "    L = sch.linkage(d, method='complete')\n",
    "    ind = sch.fcluster(L, k * d.max(), 'distance')\n",
    "    columns = [df.columns.tolist()[i] for i in list((np.argsort(ind)))]\n",
    "    df = df.reindex(columns, axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5. Features selection: genetic selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticSelector():\n",
    "    def __init__(self, estimator, n_gen, size, n_best, n_rand, \n",
    "                 n_children, mutation_rate):\n",
    "        # Estimator \n",
    "        self.estimator = estimator\n",
    "        # Number of generations\n",
    "        self.n_gen = n_gen\n",
    "        # Number of chromosomes in population\n",
    "        self.size = size\n",
    "        # Number of best chromosomes to select\n",
    "        self.n_best = n_best\n",
    "        # Number of random chromosomes to select\n",
    "        self.n_rand = n_rand\n",
    "        # Number of children created during crossover\n",
    "        self.n_children = n_children\n",
    "        # Probablity of chromosome mutation\n",
    "        self.mutation_rate = mutation_rate\n",
    "        \n",
    "        if int((self.n_best + self.n_rand) / 2) * self.n_children != self.size:\n",
    "            raise ValueError(\"The population size is not stable.\")  \n",
    "            \n",
    "    def initilize(self):\n",
    "        population = []\n",
    "        for i in range(self.size):\n",
    "            chromosome = np.ones(self.n_features, dtype=np.bool)\n",
    "            mask = np.random.rand(len(chromosome)) < 0.3\n",
    "            chromosome[mask] = False\n",
    "            population.append(chromosome)\n",
    "        return population\n",
    "\n",
    "    def fitness(self, population):\n",
    "        X, y = self.dataset\n",
    "        scores = []\n",
    "        for chromosome in population:\n",
    "            score = -1.0 * np.mean(cross_val_score(self.estimator, X[:,chromosome], y, \n",
    "                                                       cv=5, \n",
    "                                                       scoring=\"neg_mean_absolute_error\"))\n",
    "            scores.append(score)\n",
    "        scores, population = np.array(scores), np.array(population) \n",
    "        inds = np.argsort(scores)\n",
    "        return list(scores[inds]), list(population[inds,:])\n",
    "\n",
    "    def select(self, population_sorted):\n",
    "        population_next = []\n",
    "        for i in range(self.n_best):\n",
    "            population_next.append(population_sorted[i])\n",
    "        for i in range(self.n_rand):\n",
    "            population_next.append(random.choice(population_sorted))\n",
    "        random.shuffle(population_next)\n",
    "        return population_next\n",
    "\n",
    "    def crossover(self, population):\n",
    "        population_next = []\n",
    "        for i in range(int(len(population)/2)):\n",
    "            for j in range(self.n_children):\n",
    "                chromosome1, chromosome2 = population[i], population[len(population)-1-i]\n",
    "                child = chromosome1\n",
    "                mask = np.random.rand(len(child)) > 0.5\n",
    "                child[mask] = chromosome2[mask]\n",
    "                population_next.append(child)\n",
    "        return population_next\n",
    "\n",
    "    def mutate(self, population):\n",
    "        population_next = []\n",
    "        for i in range(len(population)):\n",
    "            chromosome = population[i]\n",
    "            if random.random() < self.mutation_rate:\n",
    "                mask = np.random.rand(len(chromosome)) < 0.05\n",
    "                chromosome[mask] = False\n",
    "            population_next.append(chromosome)\n",
    "        return population_next\n",
    "\n",
    "    def generate(self, population):\n",
    "        # Selection, crossover and mutation\n",
    "        scores_sorted, population_sorted = self.fitness(population)\n",
    "        population = self.select(population_sorted)\n",
    "        population = self.crossover(population)\n",
    "        population = self.mutate(population)\n",
    "        # History\n",
    "        self.chromosomes_best.append(population_sorted[0])\n",
    "        self.scores_best.append(scores_sorted[0])\n",
    "        self.scores_avg.append(np.mean(scores_sorted))\n",
    "        \n",
    "        return population\n",
    "\n",
    "    def fit(self, X, y):\n",
    " \n",
    "        self.chromosomes_best = []\n",
    "        self.scores_best, self.scores_avg  = [], []\n",
    "        \n",
    "        self.dataset = X, y\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        population = self.initilize()\n",
    "        for i in range(self.n_gen):\n",
    "            population = self.generate(population)\n",
    "            \n",
    "        return self \n",
    "    \n",
    "    @property\n",
    "    def support_(self):\n",
    "        return self.chromosomes_best[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 67 stations.\n"
     ]
    }
   ],
   "source": [
    "# Filter stations based on data availability.\n",
    "stations = dq.filter_sw_stations(DATE_FROM, DATE_TO, 1.0)\n",
    "\n",
    "# Remove lake stations...\n",
    "redundant_stations = [3280, 3350]\n",
    "stations = [s for s in stations if s not in redundant_stations]\n",
    "print(f'Selected {len(stations)} stations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-ae3fe16fc7d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-ae3fe16fc7d9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <>\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional filtering\n",
    "if TEST_STATIONS:\n",
    "    stations = TEST_STATIONS\n",
    "station_sample = stations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Watercourses data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get water data.\n",
    "water_dfs = {}\n",
    "for station in stations:\n",
    "    water_df = dq.get_sw_data(station, DATE_FROM, DATE_TO)\n",
    "    \n",
    "    # Imputate missing values.\n",
    "    if IMPUTATE:\n",
    "        water_df = imputate(water_df)\n",
    "    water_dfs[station] = water_df\n",
    "\n",
    "water_df_sample = water_dfs[station_sample]\n",
    "water_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot water levels.\n",
    "plt.figure(figsize=(13, 7))\n",
    "plt.plot(water_df_sample.index.values, water_df_sample['level'])\n",
    "plt.title(f'Water levels for station {station_sample} between {DATE_FROM[0:4]} and {DATE_TO[0:4]}')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('water level [cm]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot water level histogram.\n",
    "plt.figure(figsize=(13, 7))\n",
    "plt.hist(water_df_sample['level'])\n",
    "plt.title(f'Water levels histogram for station {station_sample} between {DATE_FROM[0:4]} and {DATE_TO[0:4]}')\n",
    "plt.xlabel('water level [cm]')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get weather data.\n",
    "weather_dfs = {}\n",
    "for station in stations:\n",
    "    weather_df = dq.get_sw_weather_data(station, DATE_FROM, DATE_TO)\n",
    "    process_weather(weather_df)\n",
    "    \n",
    "    # Drop selected features.\n",
    "    for drop_feature in DROP_FEATURES:\n",
    "        weather_df.drop(drop_feature, 1, inplace=True)\n",
    "        \n",
    "    # Imputate missing values.\n",
    "    if IMPUTATE:\n",
    "        weather_df = imputate(weather_df)\n",
    "    \n",
    "    weather_dfs[station] = weather_df\n",
    "\n",
    "weather_df_sample = weather_dfs[station_sample]\n",
    "weather_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot weather data.\n",
    "skip_list = ['day_time']\n",
    "variations_map = {\n",
    "    'avg': [1, 0],\n",
    "    'min': [2, 2], \n",
    "    'max': [4, 2],\n",
    "}\n",
    "feature_variations = variations_map.keys()\n",
    "for feature_name in list(weather_df_sample.columns.values):\n",
    "    if feature_name in skip_list:\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(13, 7))\n",
    "    plt.xlabel('date')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if feature_name[-3:] in feature_variations:\n",
    "        plt.ylabel(feature_name[:-4])\n",
    "        for fv in feature_variations:\n",
    "            fvn = feature_name[:-3] + fv\n",
    "            if fvn not in weather_df_sample.columns.to_list():\n",
    "                continue\n",
    "            skip_list.append(fvn)\n",
    "            plt.plot(weather_df_sample.index.values, weather_df_sample[fvn], dashes=variations_map[fv], label=fv)\n",
    "        #plt.ylabel(feature_name[:-3] + 'avg')\n",
    "        #plt.plot(weather_df_sample.index.values, weather_df_sample[feature_name[:-3] + 'avg'])\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.ylabel(feature_name)\n",
    "        plt.plot(weather_df_sample.index.values, weather_df_sample[feature_name], label=feature_name)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for station, water_df, weather_df in zip(water_dfs.keys(), water_dfs.values(), weather_dfs.values()):\n",
    "    water_df['level_diff'] = water_df['level'] - water_df['level'].shift(1)\n",
    "    datasets[station] = pd.concat([water_df, weather_df], axis=1)[1:]\n",
    "\n",
    "dataset_sample = datasets[station_sample]\n",
    "dataset_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show scatter plot for each feature.\n",
    "plt.figure(figsize=(17, 37))\n",
    "for index, feature_name in enumerate(dataset_sample.columns[2:]):\n",
    "    plt.subplot(9, 3, index + 1)\n",
    "    plt.scatter(dataset_sample[feature_name], dataset_sample['level'])\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('water level [cm]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = copy.deepcopy(dataset_sample.iloc[:, 1:])\n",
    "corr = ds.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrix.\n",
    "plot_corr(corr, ds.columns, size=6, file_name='correlatin_matrix_basic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot clustered correlation matrix.\n",
    "dsc = cluster_df(ds)\n",
    "plot_corr(dsc.corr(), dsc.columns, size=6, file_name='correlatin_matrix_basic_clusterred.png')\n",
    "\n",
    "dsc2 = cluster_df(ds, k=0.0)\n",
    "plot_corr(dsc2.corr(), dsc2.columns, size=6, file_name='correlatin_matrix_basic_clusterred2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.sort_values(['level_diff'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop correlated features.\n",
    "# ds.drop('temperature_avg', 1, inplace=True)\n",
    "# ds.drop('temperature_min', 1, inplace=True)\n",
    "# ds.drop('temperature_max', 1, inplace=True)\n",
    "ds.drop('dew_point_avg', 1, inplace=True)\n",
    "ds.drop('dew_point_min', 1, inplace=True)\n",
    "ds.drop('dew_point_max', 1, inplace=True)\n",
    "ds.drop('pressure_avg', 1, inplace=True)\n",
    "ds.drop('pressure_min', 1, inplace=True)\n",
    "ds.drop('pressure_max', 1, inplace=True)\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new shifted and averaged features.\n",
    "shift_features(ds, ['level', 'level_diff'])\n",
    "average_features(ds, ['level', 'level_diff'])\n",
    "\n",
    "ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate corelation matrix.\n",
    "corr = ds.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot correlation matrix.\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 16)\n",
    "im = plt.imshow(corr)\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort correlated features.\n",
    "corr.sort_values(['level_diff'], ascending = False)['level_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual feture selection:\n",
    "base_feature_names = [\n",
    "    'day_time',\n",
    "    'precipitation', # !\n",
    "    'snow_accumulation',\n",
    "    'temperature_avg',\n",
    "    'temperature_min',\n",
    "    'temperature_max',\n",
    "    'cloud_cover_avg',\n",
    "    'cloud_cover_min',\n",
    "    'cloud_cover_max',\n",
    "#     'dew_point_avg',\n",
    "#     'dew_point_min',\n",
    "#     'dew_point_max',\n",
    "    'humidity_avg', # !\n",
    "    'humidity_min',\n",
    "    'humidity_max',\n",
    "#     'pressure_avg',\n",
    "#     'pressure_min',\n",
    "#     'pressure_max',\n",
    "#     'uv_index_avg',\n",
    "#     'uv_index_min',\n",
    "#     'uv_index_max',\n",
    "    'precipitation_probability_avg', # !\n",
    "    'precipitation_probability_min', # !\n",
    "    'precipitation_probability_max', # !\n",
    "    'precipitation_intensity_avg',\n",
    "    'precipitation_intensity_min',\n",
    "    'precipitation_intensity_max', # !\n",
    "]\n",
    "\n",
    "level_feature_names = [\n",
    "#     'level'\n",
    "    \n",
    "    f'level_shift_{HORIZON}d',\n",
    "    f'level_shift_{HORIZON + 1}d',\n",
    "    f'level_shift_{HORIZON + 2}d',\n",
    "#     'level_shift_80d',\n",
    "#     'level_shift_25d',\n",
    "#     'level_shift_24d',\n",
    "#     'level_shift_16d',\n",
    "    \n",
    "#     'level_shift_3d_average_2d',\n",
    "#     'level_shift_3d_average_3d',\n",
    "#     'level_shift_3d_average_4d',\n",
    "#     'level_shift_70d_average_6d',\n",
    "#     'level_shift_70d_average_7d',\n",
    "#     'level_shift_70d_average_5d',\n",
    "#     'level_shift_70d_average_10d',\n",
    "#     'level_shift_70d_average_11d',\n",
    "    \n",
    "#     'level_diff',\n",
    "        \n",
    "#     'level_diff_shift_{HORIZON}d',\n",
    "#     'level_diff_shift_{HORIZON + 1}d',\n",
    "#     'level_diff_shift_{HORIZON + 2}d',\n",
    "#     'level_diff_shift_10d',\n",
    "#     'level_diff_shift_100d',\n",
    "#     'level_diff_shift_12d',\n",
    "#     'level_diff_shift_80d',\n",
    "#     'level_diff_shift_19d',\n",
    "    \n",
    "#     'level_diff_shift_{HORIZON}d_average_2d',\n",
    "#     'level_diff_shift_{HORIZON}d_average_3d',\n",
    "#     'level_diff_shift_{HORIZON}d_average_4d',\n",
    "#     'level_diff_shift_80d_average_21d',\n",
    "#     'level_diff_shift_80d_average_22d',\n",
    "#     'level_diff_shift_80d_average_10d',\n",
    "#     'level_diff_shift_80d_average_11d',\n",
    "#     'level_diff_shift_80d_average_20d',\n",
    "] if HORIZON > 0 else []\n",
    "\n",
    "manual_feature_names = base_feature_names + level_feature_names + [\n",
    "    \n",
    "#     'precipitation_intensity_avg_average_2d',\n",
    "#     'precipitation_average_2d',\n",
    "#     'precipitation_intensity_max_average_2d',\n",
    "#     'precipitation_probability_avg_average_2d',\n",
    "#     'precipitation_intensity_avg',\n",
    "#     'precipitation_intensity_avg_shift_1d',\n",
    "#     'precipitation_shift_1d',\n",
    "#     'precipitation_intensity_avg_average_3d',\n",
    "#     'precipitation_probability_avg_shift_1d',\n",
    "#     'precipitation_average_3d',\n",
    "#     'precipitation_intensity_max_shift_1d',\n",
    "#     'precipitation_probability_max_average_2d',\n",
    "#     'precipitation_probability_avg_average_3d',\n",
    "#     'precipitation_intensity_max_average_3d',\n",
    "#     'precipitation_probability_max_shift_1d',\n",
    "#     'precipitation_probability_max_average_3d',\n",
    "    \n",
    "# #     'precipitation_intensity_avg_average_4d',\n",
    "# #     'precipitation_average_4d',\n",
    "# #     'precipitation_probability_avg_average_4d',\n",
    "# #     'precipitation_probability_min_average_2d',\n",
    "# #     'precipitation_intensity_max_average_4d',\n",
    "# #     'precipitation_probability_avg_shift_1d_average_2d',\n",
    "# #     'precipitation_intensity_avg_shift_1d_average_2d',\n",
    "# #     'precipitation_shift_1d_average_2d',\n",
    "# #     'precipitation_intensity_max_shift_1d_average_2d',\n",
    "# #     'precipitation_probability_max_average_4d',\n",
    "# #     'precipitation_intensity_avg_average_5d',\n",
    "#     'cloud_cover_min_average_2d',\n",
    "# #     'precipitation_average_5d',\n",
    "# #     'precipitation_probability_min_average_3d',\n",
    "# #     'precipitation_probability_max_shift_1d_average_2d',\n",
    "# #     'precipitation_intensity_min_average_2d',\n",
    "#     'humidity_min_average_2d',\n",
    "#     'cloud_cover_avg_average_2d',\n",
    "#     'cloud_cover_min_shift_1d',\n",
    "# #     'precipitation_probability_avg_average_5d',\n",
    "# #     'precipitation_intensity_max_average_5d',\n",
    "#     'humidity_min_shift_1d',\n",
    "#     'humidity_avg_average_2d',\n",
    "    \n",
    "    'precipitation_average_2d',\n",
    "    'precipitation_average_3d',\n",
    "    'precipitation_average_4d',\n",
    "    'precipitation_shift_1d',\n",
    "    'precipitation_shift_1d_average_2d',\n",
    "    'precipitation_shift_1d_average_3d',\n",
    "\n",
    "    'precipitation_intensity_avg_average_2d',\n",
    "    'precipitation_intensity_avg_average_3d',\n",
    "    'precipitation_intensity_avg_average_4d',\n",
    "    'precipitation_intensity_avg_shift_1d',\n",
    "    'precipitation_intensity_avg_shift_1d_average_2d',\n",
    "    'precipitation_intensity_avg_shift_1d_average_3d',\n",
    "\n",
    "    'precipitation_probability_avg_average_2d',\n",
    "    'precipitation_probability_avg_average_3d',\n",
    "    'precipitation_probability_avg_average_4d',\n",
    "    'precipitation_probability_avg_shift_1d',\n",
    "    'precipitation_probability_avg_shift_1d_average_2d',\n",
    "    'precipitation_probability_avg_shift_1d_average_3d',\n",
    "\n",
    "    'cloud_cover_avg_average_2d',\n",
    "    'cloud_cover_avg_average_3d',\n",
    "    'cloud_cover_avg_average_4d',\n",
    "    'cloud_cover_avg_shift_1d',\n",
    "    'cloud_cover_avg_shift_1d_average_2d',\n",
    "    'cloud_cover_avg_shift_1d_average_3d',\n",
    "\n",
    "    'humidity_avg_average_2d',\n",
    "    'humidity_avg_average_3d',\n",
    "    'humidity_avg_average_4d',\n",
    "    'humidity_avg_shift_1d',\n",
    "    'humidity_avg_shift_1d_average_2d',\n",
    "    'humidity_avg_shift_1d_average_3d',\n",
    "    \n",
    "    'snow_accumulation_shift_9d',\n",
    "    'snow_accumulation_shift_7d_average_4d',\n",
    "    'snow_accumulation_shift_7d_average_2d',\n",
    "]\n",
    "\n",
    "len(manual_feature_names), manual_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(FEATURE_SELECTION_ALGORITHM==\"GENETIC\"):\n",
    "    min_row = MAX_SHIFT + MAX_AVERAGE\n",
    "    X = ds.iloc[min_row:, 2:].values.astype(float)\n",
    "    y = ds['level_diff'].iloc[min_row:].values.astype(float)\n",
    "    feature_selector = GeneticSelector(estimator=LinearRegression(), \n",
    "                          n_gen=7, size=200, n_best=40, n_rand=40, \n",
    "                          n_children=5, mutation_rate=0.05)\n",
    "    feature_selector.fit(X, y)\n",
    "    score = -1.0 * cross_val_score(est, X[:,feature_selector.support_], y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "    feature_selector.chromosomes_best\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(FEATURE_SELECTION_ALGORITHM==\"KBEST\"):\n",
    "    # Automatic feature selection:\n",
    "    min_row = MAX_SHIFT + MAX_AVERAGE\n",
    "    X = ds.iloc[min_row:, 2:].values.astype(float)\n",
    "    y = ds['level_diff'].iloc[min_row:].values.astype(float)\n",
    "    feature_selector = SelectKBest(score_func=f_regression, k=MAX_FEATURES)\n",
    "    feature_model = feature_selector.fit(X, y)\n",
    "    indices = feature_model.scores_.argsort()[-MAX_FEATURES:][::-1]\n",
    "    auto_feature_names = list(ds.columns[2:][indices])\n",
    "\n",
    "    len(auto_feature_names), auto_feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "feature_names = auto_feature_names\n",
    "\n",
    "# Build dataset with correlated features.\n",
    "ds_correlated = pd.DataFrame()\n",
    "ds_correlated['level'] = copy.deepcopy(ds['level'])\n",
    "ds_correlated['level_diff'] = copy.deepcopy(ds['level_diff'])\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    ds_correlated[feature_name] = copy.deepcopy(ds[feature_name])\n",
    "    \n",
    "ds_correlated"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ds_correlated = ds_correlated.fillna(0)\n",
    "# datasets[station_sample] = ds_correlated[10:]\n",
    "ds_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of features correlated with water level change.\n",
    "fig, ax = plt.subplots()\n",
    "fig.autofmt_xdate()\n",
    "fig.set_size_inches(10, 5)\n",
    "col_size = 500\n",
    "ax.plot(range(col_size), ds[:col_size]['level_diff'] / np.max(np.abs(ds[:col_size]['level_diff']),axis=0), label=\"Water level change\");\n",
    "ax.plot(range(col_size), ds[:col_size]['precipitation_average_3d'] / np.max(np.abs(ds[:col_size]['precipitation_average_3d']),axis=0), label=\"Precipitation avg. (3d)\");\n",
    "ax.plot(range(col_size), ds[:col_size]['cloud_cover_avg_average_4d'] / np.max(np.abs(ds[:col_size]['cloud_cover_avg_average_4d']),axis=0), label=\"Cloud cover avg. (4d)\");\n",
    "ax.legend(loc=4, borderaxespad=1)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Normalized value')\n",
    "save_figure(plt, 'feature_value_correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot highly correlated features.\n",
    "fig, ax = plt.subplots()\n",
    "fig.autofmt_xdate()\n",
    "fig.set_size_inches(10, 5)\n",
    "ax.plot(dataset_sample.index, dataset_sample.loc[:, 'temperature_avg'], label='Temperature avg.')\n",
    "ax.plot(dataset_sample.index, dataset_sample.loc[:, 'dew_point_avg'], label='Dew point avg.')\n",
    "ax.legend(loc=4, borderaxespad=1)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "save_figure(plt, 'features_correlation.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    dataset.drop('dew_point_avg', 1, inplace=True)\n",
    "    dataset.drop('dew_point_min', 1, inplace=True)\n",
    "    dataset.drop('dew_point_max', 1, inplace=True)\n",
    "    dataset.drop('pressure_avg', 1, inplace=True)\n",
    "    dataset.drop('pressure_min', 1, inplace=True)\n",
    "    dataset.drop('pressure_max', 1, inplace=True)\n",
    "\n",
    "    shift_features(dataset, ['level', 'level_diff'])\n",
    "    average_features(dataset, ['level', 'level_diff'])\n",
    "    \n",
    "    min_row = MAX_SHIFT + MAX_AVERAGE\n",
    "    datasets[key] = dataset.iloc[min_row:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = create_results_df(datasets)\n",
    "\n",
    "print(results)\n",
    "\n",
    "# 1. Run tests over all datasets:\n",
    "# ===============================\n",
    "for station_id, dataset in datasets.items():\n",
    "\n",
    "    # 2. Run tests for absolute water level and water level change:\n",
    "    # =============================================================\n",
    "    for prediction_type in [PREDICT_REL]: # [PREDICT_ABS, PREDICT_REL]:\n",
    "\n",
    "        # 3. Run tests for regression and classification:\n",
    "        # ===============================================\n",
    "        for modelling_type in [MODEL_REG]: # [MODEL_REG, MODEL_CLS]:\n",
    "                \n",
    "            # Prepare dataset.\n",
    "            X = dataset.iloc[:, 2:]\n",
    "            # X = X.values.astype(float)\n",
    "            print(f'ALL FEATURES: {len(X.iloc[0])}')\n",
    "\n",
    "            # Set dependent variable.\n",
    "            if prediction_type == PREDICT_ABS:\n",
    "                y = dataset['level']\n",
    "            else:\n",
    "                y = dataset['level_diff']\n",
    "                \n",
    "            # Discretize if necessary.\n",
    "            if modelling_type == MODEL_CLS:\n",
    "                y = discretize(y)\n",
    "\n",
    "            y = y.values.astype(float)\n",
    "\n",
    "            # Select features.\n",
    "            feature_model = SelectKBest(score_func=f_regression, k='all').fit(X.values.astype(float), y)\n",
    "            feature_indices = feature_model.scores_.argsort()[-MAX_FEATURES:][::-1]\n",
    "            features = [(dataset.columns[2:][i], feature_model.scores_[i]) for i in feature_indices]\n",
    "            selected_features = features[:54]\n",
    "#             selected_features = select_features(features, max_variations=2, max_features=54)\n",
    "            \n",
    "#             selected_features.sort(key=lambda f: f[0])\n",
    "#             print(f'SELECTED FEATURES[{len(selected_features)}]:')\n",
    "#             for feature_name, feature_score in selected_features:\n",
    "#                 print(feature_name, round(feature_score, 2))\n",
    "                \n",
    "            feature_names = [f[0] for f in selected_features]\n",
    "            feature_names = list(set(base_feature_names + level_feature_names + feature_names))\n",
    "#             feature_names = manual_feature_names\n",
    "            X = filter_columns(X, feature_names)\n",
    "#             display(HTML(X.to_html()))\n",
    "#             X[X.abs() < 0.001] = 0\n",
    "            X = X.values.astype(float)\n",
    "            \n",
    "            print(f'SELECTED FEATURES: {len(feature_names)}')\n",
    "            feature_names.sort()\n",
    "            for feature_name in feature_names:\n",
    "                print(feature_name)\n",
    "            \n",
    "\n",
    "            # 4. Run test for batch and stream models:\n",
    "            # ===========================================\n",
    "            for processing_type in [PROCESS_BAT, PROCESS_STR]:\n",
    "                \n",
    "                # Pick methods and scores.\n",
    "                print(\"METHODS[{}][{}]\".format(modelling_type, processing_type)) # TODO delete\n",
    "                \n",
    "                methods = METHODS[modelling_type][processing_type]\n",
    "                eval_score = EVAL[modelling_type][EVAL_SCORE]\n",
    "\n",
    "\n",
    "                # 5. Run tests for all selected methods:\n",
    "                # =====================================\n",
    "                for key, method in methods.items():\n",
    "\n",
    "                    # Scale features.\n",
    "                    scaler = method.get('scaler')\n",
    "                    if scaler:\n",
    "                        scaler = scaler['class'](**scaler.get('params', {}))\n",
    "                        X_tt = scaler.fit_transform(X)\n",
    "                    else:\n",
    "                        X_tt = X\n",
    "                    \n",
    "                    # Split dataset.\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X_tt, y, test_size=TEST_SIZE, shuffle=False, stratify=None)\n",
    "                    y_test_abs = dataset['level'].values.astype(float)[-len(y_test):]\n",
    "                    predicted = np.zeros(len(y_test))\n",
    "                    predicted_abs_h = np.zeros(len(y_test))\n",
    "                    \n",
    "                    fit_time = 0.0\n",
    "                    predict_time = 0.0\n",
    "                    score = 0.0\n",
    "                    score_min = float('inf')\n",
    "                    score_max = float('-inf')\n",
    "                    score_abs = 0.0\n",
    "                    score_abs_min = float('inf')\n",
    "                    score_abs_max = float('-inf')\n",
    "                    \n",
    "                    \n",
    "                    # print(type(m).__name__)\n",
    "                    method_name = ''\n",
    "                    for i in range(TEST_TRIALS):\n",
    "                        params = method.get('params', {}).copy()\n",
    "                        grid = method.get('grid', {}).copy()\n",
    "                        base_estimator = params.get('base_estimator')\n",
    "                        if (base_estimator):\n",
    "                            params['base_estimator'] = base_estimator['class'](**base_estimator.get('params', {}))\n",
    "                        \n",
    "                        m = method['class'](**params)\n",
    "                        \n",
    "                        # Learn.\n",
    "                        t = time.time()\n",
    "                        \n",
    "                        if(len(grid.keys())>0):\n",
    "                            gs = RandomizedSearchCV(estimator=m, param_distributions=grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=1)\n",
    "                            gs.fit(X_train, y_train)\n",
    "                            m = method['class'](**gs.best_params_)\n",
    "\n",
    "                        print(\"########################\")\n",
    "                        print(m)\n",
    "                        print(\"########################\")\n",
    "                        m.fit(X_train, y_train)\n",
    "                        \n",
    "                        fit_time += time.time() - t\n",
    "\n",
    "                        # Predict.\n",
    "                        t = time.time()\n",
    "                        y_predicted = m.predict(X_test)\n",
    "                        predict_time += time.time() - t\n",
    "                        \n",
    "                        if y_predicted.ndim > 1:\n",
    "                            y_predicted = np.squeeze(y_predicted)\n",
    "\n",
    "                        predicted += y_predicted\n",
    "                        \n",
    "                        # Evaluate.\n",
    "                        current_score = eval_score['class'](y_test, y_predicted, **eval_score.get('params', {}))\n",
    "                        score += current_score\n",
    "                        score_min = min(current_score, score_min)\n",
    "                        score_max = max(current_score, score_max)\n",
    "                        \n",
    "                        if HORIZON > 0:\n",
    "                            h = HORIZON\n",
    "                            y_predicted_abs_h = np.zeros(len(y_test_abs))\n",
    "                            test_len = len(y_test_abs)\n",
    "                            for i, abs_val in enumerate(y_test_abs):\n",
    "                                if i + h >= test_len:\n",
    "                                    break;\n",
    "                                y_predicted_abs_h[i + h] = abs_val + sum(y_predicted[i:i + h])\n",
    "\n",
    "                            predicted_abs_h += y_predicted_abs_h\n",
    "\n",
    "                            current_score_abs = eval_score['class'](y_test_abs[h:], y_predicted_abs_h[h:], **eval_score.get('params', {}))\n",
    "                            score_abs += current_score_abs\n",
    "                            score_abs_min = min(current_score_abs, score_abs_min)\n",
    "                            score_abs_max = max(current_score_abs, score_abs_max)\n",
    "\n",
    "                    # Store results.\n",
    "                    method_name = f'{method[\"class\"].__name__}[{key}]'\n",
    "                    index = (station_id, prediction_type, modelling_type, processing_type, method_name)\n",
    "                    results.loc[index, 'Score'] = score / TEST_TRIALS\n",
    "                    results.loc[index, 'Score Min'] = score_min\n",
    "                    results.loc[index, 'Score Max'] = score_max\n",
    "                    results.loc[index, 'Learning Time'] = fit_time / TEST_TRIALS\n",
    "                    results.loc[index, 'Predicting Time'] = predict_time / TEST_TRIALS\n",
    "                    if HORIZON > 0:\n",
    "                        results.loc[index, f'Score (h={HORIZON})'] = score_abs / TEST_TRIALS\n",
    "                        results.loc[index, f'Score Min (h={HORIZON})'] = score_abs_min\n",
    "                        results.loc[index, f'Score Max (h={HORIZON})'] = score_abs_max\n",
    "                        predicted_abs_h /= TEST_TRIALS\n",
    "\n",
    "                    predicted /= TEST_TRIALS\n",
    "                    days_len = 350 # len(y_test)\n",
    "                    days = list(range(days_len))\n",
    "\n",
    "                    # Plot predicted value differences and real value differences.\n",
    "                    fig, ax = plt.subplots()\n",
    "                    fig.set_size_inches(10, 5)\n",
    "                    ax.plot(days, y_test[:days_len], label=\"True value\")\n",
    "                    ax.plot(days, predicted[:days_len], label=\"Prediction\")\n",
    "                    ax.legend(loc=1, borderaxespad=1)\n",
    "                    plt.xlabel('Time (days)')\n",
    "                    plt.ylabel('Water level change (cm)')\n",
    "                    plt.title(method['name'])\n",
    "                    save_figure(plt, f'diff_{method_name}.png')\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot predicted values and real values.\n",
    "                    predicted_abs = [y_test_abs[0]]\n",
    "                    for n, val in enumerate(predicted[1:]):\n",
    "                        predicted_abs.append(predicted_abs[n] + val)\n",
    "\n",
    "                    fig, ax = plt.subplots() \n",
    "                    fig.set_size_inches(10, 5)\n",
    "                    ax.plot(days, y_test_abs[:days_len], label=\"True value\")\n",
    "                    ax.plot(days, predicted_abs[:days_len], label=\"Prediction\")\n",
    "                    ax.legend(loc=1, borderaxespad=1)\n",
    "                    plt.xlabel('Time (days)')\n",
    "                    plt.ylabel('Water level (cm)')\n",
    "                    plt.title(method['name'])\n",
    "                    save_figure(plt, f'sum_{method_name}.png')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    if HORIZON > 0:\n",
    "                        # Plot predicted values with prediction horizon and real values.\n",
    "                        h = HORIZON\n",
    "                        days_len_max = min(len(predicted_abs_h) - h, days_len)\n",
    "                        fig, ax = plt.subplots() \n",
    "                        fig.set_size_inches(10, 5)\n",
    "                        ax.plot(days[:days_len_max], y_test_abs[h:days_len_max + h], label=\"True value\")\n",
    "                        ax.plot(days[:days_len_max], predicted_abs_h[h:days_len_max + h], label=\"Prediction\")\n",
    "                        ax.legend(loc=1, borderaxespad=1)\n",
    "                        plt.xlabel('Time (days)')\n",
    "                        plt.ylabel('Water level (cm)')\n",
    "                        plt.title(f\"{method['name']} (h={HORIZON})\")\n",
    "                        save_figure(plt, f'sum_h{HORIZON}_{method_name}.png')\n",
    "                        plt.show()\n",
    "\n",
    "                        # Plot R² score as a function of prediction horizon\n",
    "#                         horizons = range(1, 11)\n",
    "#                         scores = []\n",
    "#                         for h in horizons:\n",
    "#                             y_predicted_abs_h = np.zeros(len(y_test_abs))\n",
    "#                             test_len = len(y_test_abs)\n",
    "#                             for i, abs_val in enumerate(y_test_abs):\n",
    "#                                 if i + h >= test_len:\n",
    "#                                     break;\n",
    "#                                 y_predicted_abs_h[i + h] = abs_val + sum(y_predicted[i:i + h])\n",
    "\n",
    "#                             scores.append(eval_score['class'](y_test_abs[h:], y_predicted_abs_h[h:], **eval_score.get('params', {}))\n",
    "\n",
    "#                         fig, ax = plt.subplots() \n",
    "#                         fig.set_size_inches(10, 5)\n",
    "#                         ax.plot(horizons, scores, label=\"True value\")\n",
    "#                         ax.legend(loc=1, borderaxespad=1)\n",
    "#                         plt.xlabel('Prediction horizon (days)')\n",
    "#                         plt.ylabel('R²')\n",
    "#                         plt.title(method['name'])\n",
    "#                         save_figure(plt, f'hor_{method_name}.png')\n",
    "#                         plt.show()\n",
    "                    \n",
    "            print(f'{station_id} > {prediction_type} > {modelling_type}:'.upper())\n",
    "            df = results.loc[(station_id, prediction_type, modelling_type)]\n",
    "            # df.sort_values(by=['Score'], ascending=False)\n",
    "            display(HTML(df.to_html()))\n",
    "            print('')\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results.\n",
    "results.to_pickle('resources/results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot score vs time.\n",
    "for processing_type in [PROCESS_BAT, PROCESS_STR]:\n",
    "    df = results.loc[(5078, 'relative', 'regression', processing_type)]\n",
    "    x = df.loc[:, 'Learning Time']\n",
    "    y = df.loc[:, 'Score']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, 'bx', markersize=12)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('R² score')\n",
    "    \n",
    "    legend = []\n",
    "    \n",
    "    for i, method_name in enumerate(df.index.values):\n",
    "        method_name = method_name[:-1].split('[')\n",
    "        plt.annotate(method_name[1], (x.iloc[i], y.iloc[i]), fontsize=10)\n",
    "        legend.append(f'{method_name[1]} - {method_name[0]}')\n",
    "        \n",
    "    save_figure(plt, f'time_scores_{processing_type}.png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    for l in legend:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load results.\n",
    "results_loaded = pd.read_pickle('resources/results.pkl')\n",
    "results_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
